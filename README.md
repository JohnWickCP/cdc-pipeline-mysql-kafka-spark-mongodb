# CDC Pipeline: MySQL -> Debezium ‚Üí Kafka ‚Üí Spark ‚Üí Redis -> MongoDB
> H·ªá th·ªëng tri·ªÉn khai ki·∫øn tr√∫c Change Data Capture (CDC) s·ª≠ d·ª•ng Debezium v√† Kafka ƒë·ªÉ stream change events t·ª´ MySQL, x·ª≠ l√Ω b·∫±ng Spark Structured Streaming v√† ƒë·ªìng b·ªô d·ªØ li·ªáu g·∫ßn real-time sang Redis (cache layer) v√† MongoDB (storage layer)

> Demo ƒë∆°n gi·∫£n Change Data Capture pipeline ƒë·ªìng b·ªô d·ªØ li·ªáu g·∫ßn real-time

---

## Tech Stack

```
MySQL 8.0         - C∆° s·ªü d·ªØ li·ªáu ngu·ªìn (port 3306)
Debezium 2.5      - CDC connector (port 8083) 
Kafka 7.5.0       - Message broker (port 9092)
Zookeeper 7.5.0   - Kafka coordination (port 2181)
Spark 3.5.0       - Stream processing (port 8080)
MongoDB 7.0       - C∆° s·ªü d·ªØ li·ªáu ƒë√≠ch (port 27017)
Redis 7.0         - Caching t√πy ch·ªçn (port 6379)
Docker 29.x       - Containerization
```

---

## Y√™u c·∫ßu h·ªá th·ªëng

- **OS**: Linux (Linux Mint 22+, Ubuntu, ...)
- **RAM**: 32GB (khuy·∫øn ngh·ªã)
- **Disk**: 50GB+ free space
- **Docker**: v29.1.3+
- **Docker Compose**: v5.0.0+

---

## Quick Start

### B∆∞·ªõc 1: Clone repo
```bash
git clone https://github.com/JohnWickCP/cdc-pipeline-mysql-kafka-spark-mongodb.git
cd cdc-pipeline-mysql-kafka-spark-mongodb
```

### B∆∞·ªõc 2: C·∫•u tr√∫c th∆∞ m·ª•c
```bash
mkdir -p {mysql,mongodb,kafka,spark,debezium,pipeline,demo,screenshots}
```

---

## Kh·ªüi ch·∫°y to√†n b·ªô Pipeline

### B∆∞·ªõc 1: Start t·∫•t c·∫£ services
```bash
cd pipeline
docker compose down -v
docker compose up -d
docker ps
```

**D·ª± ki·∫øn**: T·∫•t c·∫£ container ch·∫°y

üì∏ **·∫¢nh minh h·ªça**: `screenshots/01-full-pipeline-running.png`

![Full Pipeline Running](screenshots/01-full-pipeline-running.png)

---

## Phase 1: Kh·ªüi t·∫°o MySQL

### Kh·ªüi t·∫°o d·ªØ li·ªáu MySQL

```bash
# Ch·∫°y script kh·ªüi t·∫°o database
docker exec -i cdc-mysql mysql -uroot -proot < demo/init.sql
docker exec -i cdc-mysql mysql -uroot -proot < demo/test-data.sql
```

### Ki·ªÉm tra d·ªØ li·ªáu

```bash
docker exec -it cdc-mysql mysql -uroot -proot -e "USE inventory; SELECT * FROM customers;"
```

**D·ª± ki·∫øn**: Danh s√°ch kh√°ch h√†ng ban ƒë·∫ßu

üì∏ **·∫¢nh minh h·ªça**: `screenshots/02-mysql-initial-data.png`

![MySQL Initial Data](screenshots/02-mysql-initial-data.png)

---

## Phase 2: ƒêƒÉng k√Ω Debezium Connector

### Register MySQL Connector

```bash
cd demo
./register-connector.sh
```

### Ki·ªÉm tra Connector Status

```bash
# Li·ªát k√™ t·∫•t c·∫£ connectors
curl http://localhost:8083/connectors

# Ki·ªÉm tra tr·∫°ng th√°i connector
curl http://localhost:8083/connectors/mysql-connector/status
```

**Expected output**:
```json
{
  "name": "mysql-connector",
  "config": {...},
  "tasks": [...],
  "type": "source"
}
```

**Tr·∫°ng th√°i mong ƒë·ª£i**:
```json
"state": "RUNNING"
```

üì∏ **·∫¢nh minh h·ªça**: `screenshots/03-debezium-connector-running.png`

![Debezium Connector Running](screenshots/03-debezium-connector-running.png)

---

## Phase 3: Ki·ªÉm tra CDC (Thao t√°c CRUD)

### Topic ƒë∆∞·ª£c t·∫°o
```
mysql.inventory.customers
```

### 3.1 - ƒê·ªçc Snapshot (op=r)

Khi Debezium connector ƒëƒÉng k√Ω, n√≥ s·∫Ω ƒë·ªçc d·ªØ li·ªáu ban ƒë·∫ßu t·ª´ MySQL.

**ƒê·ªãnh d·∫°ng event**:
```json
{
  "op": "r",
  "before": null,
  "after": {
    "id": 1,
    "name": "John",
    "email": "john@example.com"
  }
}
```

### 3.2 - Thao t√°c Th√™m (op=c)

```bash
docker exec -it cdc-mysql mysql -uroot -proot -e \
"USE inventory; INSERT INTO customers (name, email) VALUES ('Eve', 'eve@example.com');"
```

**ƒê·ªçc message t·ª´ Kafka**:
```bash
docker exec -it cdc-kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic mysql.inventory.customers \
  --from-beginning
```

**ƒê·ªãnh d·∫°ng event**:
```json
{
  "op": "c",
  "before": null,
  "after": {
    "id": 5,
    "name": "Eve",
    "email": "eve@example.com"
  }
}
```

üì∏ **·∫¢nh minh h·ªça**: `screenshots/04-kafka-cdc-insert-event.png`

![Kafka CDC Insert Event](screenshots/04-kafka-cdc-insert-event.png)

### 3.3 - Thao t√°c C·∫≠p nh·∫≠t (op=u)

```bash
docker exec -it cdc-mysql mysql -uroot -proot -e \
"USE inventory; UPDATE customers SET name='Eve Updated' WHERE name='Eve';"
```

**ƒê·ªãnh d·∫°ng event**:
```json
{
  "op": "u",
  "before": {
    "name": "Eve"
  },
  "after": {
    "id": 5,
    "name": "Eve Updated",
    "email": "eve@example.com"
  }
}
```

üì∏ **·∫¢nh minh h·ªça**: `screenshots/05-kafka-cdc-update-event.png`

![Kafka CDC Update Event](screenshots/05-kafka-cdc-update-event.png)

### 3.4 - Thao t√°c X√≥a (op=d)

```bash
docker exec -it cdc-mysql mysql -uroot -proot -e \
"USE inventory; DELETE FROM customers WHERE name='Eve Updated';"
```

**ƒê·ªãnh d·∫°ng event**:
```json
{
  "op": "d",
  "before": {
    "id": 5,
    "name": "Eve Updated",
    "email": "eve@example.com"
  },
  "after": null
}
```

üì∏ **·∫¢nh minh h·ªça**: `screenshots/06-kafka-cdc-delete-event.png`

![Kafka CDC Delete Event](screenshots/06-kafka-cdc-delete-event.png)

---

## Phase 4: Spark Cluster

### Giao di·ªán Spark Master

```
URL: http://localhost:8080
```

Ki·ªÉm tra:
- Tr·∫°ng th√°i Master
- C√°c Worker ƒëang ch·∫°y (worker-1, worker-2)
- C√°c ·ª©ng d·ª•ng ƒëang ch·∫°y
- C√°c ·ª©ng d·ª•ng ƒë√£ ho√†n th√†nh

üì∏ **·∫¢nh minh h·ªça**: `screenshots/07-spark-master-ui.png`

![Spark Master UI](screenshots/07-spark-master-ui.png)

### Spark Streaming Job

```bash
# T·∫°o submission script
spark-submit \
  --class org.apache.spark.streaming.kafka010.KafkaWordCount \
  --master spark://localhost:7077 \
  --total-executor-cores 2 \
  path/to/application.jar
```

üì∏ **·∫¢nh minh h·ªça**: `screenshots/08-spark-streaming-job.png`

![Spark Streaming Job](screenshots/08-spark-streaming-job.png)

---

## Phase 5: X√°c minh Data Flow

### Ki·ªÉm tra d·ªØ li·ªáu t·ª´ MySQL ƒë·∫øn MongoDB

```bash
# 1. Th√™m d·ªØ li·ªáu v√†o MySQL
docker exec -it cdc-mysql mysql -uroot -proot -e \
"USE inventory; INSERT INTO customers (name, email) VALUES ('Test User', 'test@example.com');"

# 2. Xem event trong Kafka
docker exec -it cdc-kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic mysql.inventory.customers \
  --max-messages 10

# 3. X√°c minh trong MongoDB
docker exec -it cdc-mongodb mongosh -u root -p root123 --authenticationDatabase admin inventory -e \
"db.customers.find().pretty()"
```

**D·ª± ki·∫øn**: D·ªØ li·ªáu xu·∫•t hi·ªán trong MongoDB trong v√†i gi√¢y

üì∏ **·∫¢nh minh h·ªça**: `screenshots/09-full-pipeline-data-flow.png`

![Full Pipeline Data Flow](screenshots/09-full-pipeline-data-flow.png)

---

## C·∫•u tr√∫c Project

```
cdc-pipeline/
‚îÇ
‚îú‚îÄ‚îÄ demo/
‚îÇ   ‚îú‚îÄ‚îÄ init.sql                    # Schema + d·ªØ li·ªáu ban ƒë·∫ßu
‚îÇ   ‚îú‚îÄ‚îÄ test-data.sql               # D·ªØ li·ªáu m·∫´u
‚îÇ   ‚îî‚îÄ‚îÄ register-connector.sh        # ƒêƒÉng k√Ω Debezium connector
‚îÇ
‚îú‚îÄ‚îÄ pipeline/
‚îÇ   ‚îî‚îÄ‚îÄ docker-compose.yml          # Full stack docker-compose
‚îÇ
‚îú‚îÄ‚îÄ screenshots/
‚îÇ   ‚îú‚îÄ‚îÄ 01-full-pipeline-running.png
‚îÇ   ‚îú‚îÄ‚îÄ 02-mysql-initial-data.png
‚îÇ   ‚îú‚îÄ‚îÄ 03-debezium-connector-running.png
‚îÇ   ‚îú‚îÄ‚îÄ 04-kafka-cdc-insert-event.png
‚îÇ   ‚îú‚îÄ‚îÄ 05-kafka-cdc-update-event.png
‚îÇ   ‚îú‚îÄ‚îÄ 06-kafka-cdc-delete-event.png
‚îÇ   ‚îú‚îÄ‚îÄ 07-spark-master-ui.png
‚îÇ   ‚îú‚îÄ‚îÄ 08-spark-streaming-job.png
‚îÇ   ‚îî‚îÄ‚îÄ 09-full-pipeline-data-flow.png
‚îÇ
‚îú‚îÄ‚îÄ README.md                        # File n√†y
‚îî‚îÄ‚îÄ .gitignore
```

---

## L·ªánh h·ªØu √≠ch

### Qu·∫£n l√Ω Docker

```bash
# Ki·ªÉm tra containers
docker ps
docker ps -a

# Xem logs
docker logs cdc-debezium -f
docker logs cdc-kafka -f
docker logs cdc-spark-master -f

# X√≥a to√†n b·ªô h·ªá th·ªëng
docker compose down -v
```

### Topics Kafka

```bash
# Li·ªát k√™ topics
docker exec cdc-kafka kafka-topics \
  --bootstrap-server localhost:9092 \
  --list

# T·∫°o topic
docker exec cdc-kafka kafka-topics \
  --bootstrap-server localhost:9092 \
  --create \
  --topic <topic-name> \
  --partitions 1 \
  --replication-factor 1

# X√≥a topic
docker exec cdc-kafka kafka-topics \
  --bootstrap-server localhost:9092 \
  --delete \
  --topic <topic-name>

# Xem s·ªë message
docker exec cdc-kafka kafka-run-class kafka.tools.JmxTool \
  --object-name kafka.server:type=BrokerTopicMetrics,name=MessagesInPerSec
```

### L·ªánh MySQL

```bash
# K·∫øt n·ªëi
docker exec -it cdc-mysql mysql -uroot -proot

# Ki·ªÉm tra binlog
SHOW MASTER STATUS;

# Xem b·∫£ng
SHOW TABLES;

# Xem d·ªØ li·ªáu
SELECT * FROM customers;
```

### L·ªánh MongoDB

```bash
# K·∫øt n·ªëi
docker exec -it cdc-mongodb mongosh -u root -p root123 --authenticationDatabase admin

# Li·ªát k√™ c∆° s·ªü d·ªØ li·ªáu
show dbs

# Ch·ªçn c∆° s·ªü d·ªØ li·ªáu
use inventory

# Li·ªát k√™ collections
show collections

# Xem d·ªØ li·ªáu
db.customers.find().pretty()

# ƒê·∫øm documents
db.customers.countDocuments()
```

### Debezium REST API

```bash
# Li·ªát k√™ connectors
curl http://localhost:8083/connectors

# L·∫•y tr·∫°ng th√°i connector
curl http://localhost:8083/connectors/mysql-connector/status

# L·∫•y config connector
curl http://localhost:8083/connectors/mysql-connector/config

# T·∫°m d·ª´ng connector
curl -X PUT http://localhost:8083/connectors/mysql-connector/pause

# Ti·∫øp t·ª•c connector
curl -X PUT http://localhost:8083/connectors/mysql-connector/resume

# X√≥a connector
curl -X DELETE http://localhost:8083/connectors/mysql-connector
```

---

## ·∫¢nh minh h·ªça

## ·∫¢nh minh h·ªça

| Phase | ·∫¢nh | M√¥ t·∫£ |
|-------|-----|-------|
| 1 | `01-full-pipeline-running.png` | T·∫•t c·∫£ containers ch·∫°y |
| 2 | `02-mysql-initial-data.png` | MySQL d·ªØ li·ªáu kh·ªüi t·∫°o |
| 3 | `03-debezium-connector-running.png` | Debezium connector ƒë√£ ƒëƒÉng k√Ω |
| 4 | `04-kafka-cdc-insert-event.png` | CDC Insert event |
| 5 | `05-kafka-cdc-update-event.png` | CDC Update event |
| 6 | `06-kafka-cdc-delete-event.png` | CDC Delete event |
| 7 | `07-spark-master-ui.png` | Spark Master dashboard |
| 8 | `08-spark-streaming-job.png` | Spark streaming job |
| 9 | `09-full-pipeline-data-flow.png` | X√°c minh data flow |

---

### Chi ti·∫øt ·∫£nh minh h·ªça

#### Phase 1: Full Pipeline Running
![Full Pipeline Running](screenshots/01-full-pipeline-running.png)
*T·∫•t c·∫£ containers ch·∫°y (MySQL, MongoDB, Zookeeper, Kafka, Debezium, Spark Master, Spark Workers)*

#### Phase 2: MySQL Initial Data
![MySQL Initial Data](screenshots/02-mysql-initial-data.png)
*D·ªØ li·ªáu kh·ªüi t·∫°o trong MySQL inventory database*

#### Phase 3: Debezium Connector Running
![Debezium Connector Running](screenshots/03-debezium-connector-running.png)
*Debezium connector ƒë√£ ƒëƒÉng k√Ω v√† ch·∫°y*

#### Phase 4: Kafka CDC Insert Event
![Kafka CDC Insert Event](screenshots/04-kafka-cdc-insert-event.png)
*Event insert ƒë∆∞·ª£c ghi v√†o Kafka topic*

#### Phase 5: Kafka CDC Update Event
![Kafka CDC Update Event](screenshots/05-kafka-cdc-update-event.png)
*Event update ƒë∆∞·ª£c ghi v√†o Kafka topic*

#### Phase 6: Kafka CDC Delete Event
![Kafka CDC Delete Event](screenshots/06-kafka-cdc-delete-event.png)
*Event delete ƒë∆∞·ª£c ghi v√†o Kafka topic*

#### Phase 7: Spark Master UI
![Spark Master UI](screenshots/07-spark-master-ui.png)
*Spark Master dashboard v·ªõi workers v√† jobs*

#### Phase 8: Spark Streaming Job
![Spark Streaming Job](screenshots/08-spark-streaming-job.png)
*Spark streaming job consuming t·ª´ Kafka*

#### Phase 9: Full Pipeline Data Flow
![Full Pipeline Data Flow](screenshots/09-full-pipeline-data-flow.png)
*Data flow t·ª´ MySQL ‚Üí Kafka ‚Üí MongoDB*

---

## Giao di·ªán Web

| Service | URL | Port | M·ª•c ƒë√≠ch |
|---------|-----|------|---------|
| Kafka Control Center | http://localhost:9021 | 9021 | Qu·∫£n l√Ω Kafka topics |
| Spark Master | http://localhost:8080 | 8080 | Qu·∫£n l√Ω Spark jobs |
| Debezium REST API | http://localhost:8083 | 8083 | Qu·∫£n l√Ω connectors |
| Mongo Express | http://localhost:8081 | 8081 | Duy·ªát MongoDB |

---

## Kh·∫Øc ph·ª•c s·ª± c·ªë

### V·∫•n ƒë·ªÅ Kafka

**V·∫•n ƒë·ªÅ**: Topic kh√¥ng xu·∫•t hi·ªán  
**Gi·∫£i ph√°p**: ƒê·∫£m b·∫£o Debezium connector ƒë√£ ƒëƒÉng k√Ω v√† ch·∫°y
```bash
curl http://localhost:8083/connectors/mysql-connector/status
```

**V·∫•n ƒë·ªÅ**: Messages kh√¥ng ch·∫£y  
**Gi·∫£i ph√°p**: Ki·ªÉm tra logs Debezium
```bash
docker logs cdc-debezium -f | grep ERROR
```

### V·∫•n ƒë·ªÅ MongoDB

**V·∫•n ƒë·ªÅ**: Insert kh√¥ng th√†nh c√¥ng  
**Gi·∫£i ph√°p**: X√°c minh MongoDB ch·∫°y
```bash
docker exec -it cdc-mongodb mongosh -u root -p root123 --authenticationDatabase admin --eval "db.adminCommand('ping')"
```

### V·∫•n ƒë·ªÅ Spark

**V·∫•n ƒë·ªÅ**: Spark job l·ªói  
**Gi·∫£i ph√°p**: Ki·ªÉm tra logs Spark worker
```bash
docker logs cdc-spark-worker-1 -f
```

### Xung ƒë·ªôt c·ªïng

```bash
# T√¨m process s·ª≠ d·ª•ng c·ªïng
sudo lsof -i :<port>

# T·∫Øt process
sudo kill -9 <PID>
```

---

## Quy ∆∞·ªõc Commit

**ƒê·ªãnh d·∫°ng**: `<type>(<scope>): <subject>`

V√≠ d·ª•:
```
feat(mysql): kh·ªüi t·∫°o database v·ªõi d·ªØ li·ªáu m·∫´u
feat(debezium): ƒëƒÉng k√Ω mysql connector
feat(kafka): ki·ªÉm tra thao t√°c CRUD
feat(spark): t·∫°o streaming job
feat(pipeline): t√≠ch h·ª£p ƒë·∫ßy ƒë·ªß ho√†n th√†nh
fix(debezium): c·∫≠p nh·∫≠t c·∫•u h√¨nh connector
docs(readme): th√™m ho√†n th√†nh phase
test(pipeline): x√°c minh end-to-end data flow
```

---

## T√†i li·ªáu tham kh·∫£o

- [Docker Documentation](https://docs.docker.com/)
- [MySQL Binlog Documentation](https://dev.mysql.com/doc/refman/8.0/en/binary-log.html)
- [Debezium Documentation](https://debezium.io/documentation/)
- [Apache Kafka Documentation](https://kafka.apache.org/documentation/)
- [Apache Spark Streaming](https://spark.apache.org/docs/latest/streaming-programming-guide.html)
- [MongoDB Documentation](https://docs.mongodb.com/)

---

## Danh s√°ch ki·ªÉm tra - Full Pipeline

- [x] MySQL ƒë√£ kh·ªüi t·∫°o v·ªõi binary log
- [x] Debezium connector ƒë√£ ƒëƒÉng k√Ω
- [x] Kafka topics ƒë∆∞·ª£c t·∫°o t·ª± ƒë·ªông
- [x] CDC operations ƒë√£ ki·ªÉm tra (Insert, Update, Delete)
- [x] Spark cluster ch·∫°y
- [x] Spark streaming job consuming t·ª´ Kafka
- [x] D·ªØ li·ªáu ƒë·ªìng b·ªô v·ªõi MongoDB
- [x] Screenshots ƒë√£ ch·ª•p
- [x] Documentation ho√†n th√†nh
- [x] GitHub push ho√†n th√†nh

---

## Git Workflow

### Workflow theo Phase

```bash
# 1. Th·ª±c hi·ªán thay ƒë·ªïi
# [t·∫°o/c·∫≠p nh·∫≠t files]

# 2. Th√™m files
git add <files>

# 3. Commit
git commit -m "feat(<scope>): m√¥ t·∫£"

# 4. Push
git push origin main

# 5. X√°c minh
git log --oneline -2
```

### V√≠ d·ª• Commits

```bash
git add demo/init.sql demo/test-data.sql
git commit -m "feat(mysql): kh·ªüi t·∫°o database v√† test data"
git push origin main

git add demo/register-connector.sh
git commit -m "feat(debezium): t·∫°o script ƒëƒÉng k√Ω connector"
git push origin main

git add screenshots/04-kafka-cdc-insert-event.png
git commit -m "test(kafka): x√°c minh insert CDC event"
git push origin main
```

---

## T√≥m t·∫Øt

**Ki·∫øn tr√∫c Pipeline**:
```
MySQL (binlog enabled)
    ‚Üì
Debezium Connector
    ‚Üì
Kafka Broker (Topics)
    ‚Üì
Spark Streaming
    ‚Üì
MongoDB (Target DB)
```

**T√≠nh nƒÉng**:
- Real-time CDC v·ªõi MySQL binary log
- Kafka message broker ƒë·ªÉ streaming events
- Debezium ƒë·ªÉ seamless CDC
- Spark Streaming ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu
- MongoDB l√†m c∆° s·ªü d·ªØ li·ªáu ƒë√≠ch
- Docker Compose ƒë·ªÉ tri·ªÉn khai d·ªÖ d√†ng
- T·ª± ƒë·ªông ho√†n to√†n v·ªõi m·ªôt l·ªánh duy nh·∫•t

**Th√†nh t·ª±u ch√≠nh**:
- Pipeline ch·∫°y ho√†n to√†n qua Docker Compose
- T·∫•t c·∫£ containers ƒë∆∞·ª£c ƒëi·ªÅu ph·ªëi trong m·ªôt file
- CRUD operations ƒë∆∞·ª£c ghi l·∫°i v√† streaming
- D·ªØ li·ªáu ƒë·ªìng b·ªô ƒë∆∞·ª£c x√°c minh end-to-end
- Documentation ho√†n ch·ªânh v·ªõi ·∫£nh minh h·ªça

---
